{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mistralai/Mixtral-8x7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"/mnt/d/Code/tokenGPT/training_data/tokenGPT-KnowledgeBase/blockchain economic systems modelling.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniswap subgraphs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from dune_client.client import DuneClient\n",
    "import datetime\n",
    "from pytz import utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniswap_top_pools(n_pools):\n",
    "    \n",
    "    url = 'https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v3'\n",
    "\n",
    "    # GraphQL query to fetch the pool id and daily volume for the top n_pools\n",
    "    query = \"\"\"\n",
    "    {{\n",
    "      pools(first: {0}, orderBy: volumeUSD, orderDirection: desc) {{\n",
    "        id\n",
    "        volumeUSD\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\".format(n_pools)\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.post(url, json={'query': query})\n",
    "\n",
    "    # Get the JSON data from the response\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    # Get the pool IDs from the data\n",
    "    pool_ids = [pool['id'] for pool in data['data']['pools']]\n",
    "\n",
    "    return pool_ids\n",
    "\n",
    "def get_uniswap_pool_day_data(pool_id, number_of_days):\n",
    "  \n",
    "    url = 'https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v3'\n",
    "    query = f\"\"\"\n",
    "    {{\n",
    "        poolDayDatas(where: {{ pool: \"{pool_id}\" }}, first: {number_of_days}, orderBy: date, orderDirection: desc) {{\n",
    "            date\n",
    "            tick\n",
    "            sqrtPrice\n",
    "            liquidity\n",
    "            volumeUSD\n",
    "            volumeToken0\n",
    "            volumeToken1\n",
    "            tvlUSD\n",
    "            feesUSD\n",
    "            close\n",
    "            open\n",
    "            low\n",
    "            high\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = requests.post(url, json={'query': query})\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data['data']['poolDayDatas'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_data_to_csv(data, file_path):\n",
    "    keys = data[0].keys()\n",
    "    with open(file_path, 'w', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "def preprocess_uniswap_subgraph_data(data_df):\n",
    "    data_df['volumeUSD'] = data_df['volumeUSD'].astype(float)\n",
    "    data_df['volumeToken0'] = data_df['volumeToken0'].astype(float)\n",
    "    data_df['volumeToken1'] = data_df['volumeToken1'].astype(float)\n",
    "    data_df['sqrtPrice'] = (data_df['sqrtPrice'].astype(float))\n",
    "    data_df['liquidity'] = data_df['liquidity'].astype(float)\n",
    "    data_df['tvlUSD'] = data_df['tvlUSD'].astype(float)\n",
    "    data_df['feesUSD'] = data_df['feesUSD'].astype(float)\n",
    "    data_df['close'] = data_df['close'].astype(float)\n",
    "    data_df['open'] = data_df['open'].astype(float)\n",
    "    data_df['low'] = data_df['low'].astype(float)\n",
    "    data_df['high'] = data_df['high'].astype(float)\n",
    "    \n",
    "      # Convert date from UNIX timestamp to datetime format\n",
    "    data_df['date'] = pd.to_datetime(data_df['date'], unit='s')\n",
    "    # 7 days Rolling averages\n",
    "    data_df['feesUSD'] = data_df['feesUSD'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "    data_df['date'] = pd.to_datetime(data_df['date'], unit='s')\n",
    "  \n",
    "# Check for missing data in the control group\n",
    "    missing_values = data_df.isnull().sum()\n",
    "    missing_control = missing_values[missing_values > 0]\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def explore_uniswap_pool_data(df):\n",
    "    #Raw data analysis\n",
    "\n",
    "    df.plot(x='date', y='sqrtPrice')\n",
    "\n",
    "    df.plot(x='date', y='feesUSD')\n",
    "    df.plot(x='date', y='liquidity')\n",
    "\n",
    "    ax = df.plot(x='date', y='liquidity', color='blue', label='liquidity')\n",
    "    df.plot(x='date', y='sqrtPrice', color='red', secondary_y=True, ax=ax, label='sqrtPrice')\n",
    "\n",
    "    ax.set_ylabel('liquidity')\n",
    "    ax.right_ax.set_ylabel('sqrtPrice')\n",
    "    plt.title('liquidity and sqrtPrice Over Time')\n",
    "    plt.show()\n",
    "\n",
    "    ax = df.plot(x='date', y='liquidity', color='blue', label='liquidity')\n",
    "    df.plot(x='date', y='feesUSD', color='red', secondary_y=True, ax=ax, label='Fees')\n",
    "\n",
    "    ax.set_ylabel('liquidity')\n",
    "    ax.right_ax.set_ylabel('Fees')\n",
    "    plt.title('liquidity and Fees Over Time')\n",
    "    plt.show()\n",
    "\n",
    "    ax = df.plot(x='date', y='volumeToken0', color='blue', label='volumeToken0')\n",
    "    df.plot(x='date', y='volumeToken1', color='red', secondary_y=True, ax=ax, label='volumeToken1')\n",
    "\n",
    "    ax.set_ylabel('volumeToken0')\n",
    "    ax.right_ax.set_ylabel('volumeToken1')\n",
    "    plt.title('liquidity and Fees Over Time')\n",
    "    plt.show()\n",
    "\n",
    "    ax = df.plot(x='date', y='sqrtPrice', color='blue', label='sqrtPrice')\n",
    "    df.plot(x='date', y='sqrtPrice', color='red', secondary_y=True, ax=ax, label='sqrtPrice')\n",
    "    ax.set_ylabel('sqrtPrice')\n",
    "    ax.right_ax.set_ylabel('sqrtPrice')\n",
    "    plt.title('liquidity and Fees Over Time')\n",
    "    plt.show()\n",
    "\n",
    "    #EDA\n",
    "    plt.hist(df['volumeUSD'])\n",
    "    plt.show()\n",
    "    sns.boxplot(x=df['volumeUSD'])\n",
    "    plt.show()\n",
    "    plt.scatter(x=df['date'],y=df['volumeUSD'])\n",
    "    plt.show()\n",
    "    sns.countplot(df['volumeUSD'])\n",
    "    plt.show()\n",
    "    sns.heatmap(df.corr())\n",
    "    plt.show()\n",
    "    corr_matrix=df.corr()\n",
    "    sns.heatmap(corr_matrix,annot=True,cmap='coolwarm')\n",
    "\n",
    "    #zscore\n",
    "    df['zscore']=zscore(df['volumeUSD'])\n",
    "    outliers=df[(df['zscore']>3) | (df['zscore']<-3)]\n",
    "    print(outliers)\n",
    "\n",
    "    #transformations\n",
    "    df['log_transformed']=np.log(df['volumeUSD'])\n",
    "\n",
    "    #standardization\n",
    "    scaler = StandardScaler()\n",
    "    df['standardized_volume'] = scaler.fit_transform(df['volumeUSD'].values.reshape(-1, 1))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_uniswap_top_pools_daily_data(n_pools=5,days=30):\n",
    "    def get_uniswap_pool_daily_data(pool_id, number_of_days):\n",
    "        url = 'https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v3'\n",
    "        query = f\"\"\"\n",
    "        {{\n",
    "            poolDayDatas(where: {{ pool: \"{pool_id}\" }}, first: {number_of_days}, orderBy: date, orderDirection: desc) {{\n",
    "                date\n",
    "                sqrtPrice\n",
    "                liquidity\n",
    "                volumeUSD\n",
    "                tvlUSD\n",
    "                feesUSD\n",
    "                pool {{\n",
    "                    token0 {{\n",
    "                        symbol\n",
    "                    }}\n",
    "                    token1 {{\n",
    "                        symbol\n",
    "                    }}\n",
    "                    feeTier\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        response = requests.post(url, json={'query': query})\n",
    "        data = response.json()\n",
    "        \n",
    "        for poolDayData in data['data']['poolDayDatas']:\n",
    "            poolDayData['pool_name'] = poolDayData['pool']['token0']['symbol'] + \"/\" + poolDayData['pool']['token1']['symbol'] + \"(\" + str(poolDayData['pool']['feeTier']) + \")\"\n",
    "            poolDayData['feesUSD'] = float(poolDayData['feesUSD'])\n",
    "            poolDayData['tvlUSD'] = float(poolDayData['tvlUSD'])\n",
    "            poolDayData['volumeUSD'] = float(poolDayData['volumeUSD'])\n",
    "            poolDayData['date'] = pd.to_datetime(poolDayData['date'], unit='s')\n",
    "\n",
    "        return data['data']['poolDayDatas']\n",
    "\n",
    "\n",
    "    pool_ids=get_uniswap_top_pools(n_pools)\n",
    "    uniswap_pools_daily_data={}\n",
    "    for pool_id in pool_ids:\n",
    "        uniswap_data = get_uniswap_pool_daily_data(pool_id, days)\n",
    "        df = pd.DataFrame(uniswap_data)\n",
    "        df.drop('pool', axis=1, inplace=True)\n",
    "        uniswap_pools_daily_data[pool_id] = df\n",
    "    return uniswap_pools_daily_data\n",
    "\n",
    "def calculate_pool_performance_matrices(uniswap_pools_daily_data):\n",
    "    results = {}\n",
    "    for pool_id, df in uniswap_pools_daily_data.items():\n",
    "        \n",
    "        df = df[(df['feesUSD'] != 0) & (df['tvlUSD'] != 0) & df['feesUSD'].notna() & df['tvlUSD'].notna()]\n",
    "        \n",
    "        pool_name = df['pool_name'].iloc[0]\n",
    "        avg_feeUSD = df['feesUSD'].ewm(span=30).mean().iloc[-1]\n",
    "        avg_tvl = df['tvlUSD'].ewm(span=30).mean().iloc[-1]\n",
    "        \n",
    "        adv = df['volumeUSD'].ewm(span=30).mean().iloc[-1]\n",
    "\n",
    "        daily_roi = (df['feesUSD'] /df['tvlUSD']).ewm(span=30).mean().iloc[-1]\n",
    "        monthly_roi = ((1 + daily_roi)**30) - 1\n",
    "        annual_roi = ((1 + daily_roi)**365) - 1\n",
    "       \n",
    "        results[pool_id] = {'pool_name':pool_name,'avg_feesUSD': avg_feeUSD, 'avg_tvlUSD': avg_tvl, 'adv': adv, 'daily_roi': daily_roi,'monthly_roi':monthly_roi,'annual_roi':annual_roi}\n",
    "    df_rois_uniswap = pd.DataFrame.from_dict(results, orient='index')\n",
    "    return df_rois_uniswap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_uniswap_top_pools(5)\n",
    "df=get_uniswap_pool_day_data('0x88e6a0c2ddd26feeb64f039a2c41296fcb3f5640',10)\n",
    "df=preprocess_uniswap_subgraph_data(df)\n",
    "explore_uniswap_pool_data(df)\n",
    "data=get_uniswap_top_pools_daily_data(3,2)\n",
    "calculate_pool_performance_matrices(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dune Analytics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_query_result(query_id):\n",
    "    dune_api_key = \"gqBRKclPQMlU9Jm009ikKIrYV4gWhtuq\"\n",
    "    dune = DuneClient(dune_api_key)\n",
    "    #curl -H \"X-Dune-API-Key:\" gqBRKclPQMlU9Jm009ikKIrYV4gWhtuq\"https://api.dune.com/api/v1/query/3467177/results?limit=1000\"\n",
    "    # Fetch the latest query result\n",
    "    query_result = dune.get_latest_result(query_id)\n",
    "    return query_result.result.rows if query_result.result.rows else []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'date': '2024-05-16 23:09:00.000 UTC', 'price': 0.014633}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = {\n",
    "    \"latest_price\": 3466806,\n",
    "    \"all_contract_tx\": 3467027,\n",
    "    \"trade_data\": 3467093,\n",
    "    \"mydpt_stats\":3478340,\n",
    "    \"keys_buy_sell\":3468401,\n",
    "}\n",
    "fetch_query_result(3466806)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETH Maianet data from Alchemy/Infura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETH Maianet data from Etherscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defillama Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lping Vs Lending Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     Initial SyncSwap ETH/USDC price = 1875.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     Pool TVL = $60.0 million\n",
      "\n",
      "INFO     User 1 LPs 1 ETH. User 2 LPs 1875.0 USDC.\n",
      "INFO     Users together own 0.006249609399412537% of the total pool\n",
      "INFO     First few simulated end results:\n",
      "INFO     USDC pool amounts = [18814855. 24135141. 29466320. 35218753.]\n",
      "INFO     ETH pool amounts = [15949. 19183. 14597. 11509.]\n",
      "INFO     ETH/USDC prices = [1180. 1258. 2019. 3060.]\n",
      "INFO     Pool TVLs = $[37.62971  48.270282 58.93264  70.437506] million\n",
      "INFO     Initial pool composition: 30000000 USDC, 16000 ETH\n",
      "INFO     Initial price of ETH/USDC = 1875.0\n",
      "\n",
      "INFO     1. Alice and Bob LP 1875.0 USDC and 1 ETH\n",
      "INFO        Initially, they own 0.006249609399412537% of the total pool\n",
      "INFO     2. Other users removed liquidity: 6281704.69 USDC, 3350.24 ETH\n",
      "INFO     3. Other users bought 4905315.31 USDC for 3298.0 ETH\n",
      "\n",
      "INFO     Final pool composition: 18814855.0 USDC, 15949.0 ETH\n",
      "INFO     Final price of ETH/USDC = 1179.69\n",
      "INFO     Ultimately, the specified LPs own 0.007904664998480842% of the total pool\n",
      "\n",
      "INFO     Benchmark = ReactorFusion pools: Assume 6.5% APR on USDC and 4.5% APR on ETH\n",
      "INFO     Simply lending in ReactorFusion would yield 1996.88 USDC and 1.04 ETH\n",
      "\n",
      "INFO     Assume a conservative 11.0% APR on the SyncSwap USDC/ETH pool\n",
      "INFO     Without a rebalancing mechanism, Alice and Bob withdraw 1650.85 USDC and 1.4 ETH from SyncSwap\n",
      "\n",
      "INFO     Rebalancing mechanism: We swap 0.33 ETH for 389.84 USDC\n",
      "INFO     After the rebalancing mechanism, Alice withdraws 2040.69 USDC and Bob withdraws 1.07 ETH from SyncSwap\n",
      "\n",
      "INFO     ---------------\n",
      "INFO     Alice received 2.194106062032919% more USDC via single-sided SyncSwap LP than with ReactorFusion lending USDC alone.\n",
      "INFO     Bob received 2.194511789256981% more ETH via single-sided SyncSwap LP than with ReactorFusion lending ETH alone.\n",
      "INFO     ---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lping_vs_lending_analysis(usdc_pool_0 = 30_000_000,eth_pool_0 = 16_000,syncswap_apr = 0.11,reactorfusion_usdc_apr = 0.065,reactorfusion_eth_apr = 0.045):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set_theme()\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)-8s %(message)s')\n",
    "    logger = logging.getLogger('Sim')\n",
    "\n",
    "     # 30 million USDC in SyncSwap pool initially\n",
    "     # 16 thousand ETH in SyncSwap pool initially\n",
    "    initial_price = usdc_pool_0 / eth_pool_0\n",
    "    logger.info(f'Initial SyncSwap ETH/USDC price = {usdc_pool_0 / eth_pool_0}')\n",
    "    logger.info(f'Pool TVL = ${usdc_pool_0 * 2 / 1e6} million\\n')\n",
    "\n",
    "    eth_lp_amount = 1 # User LPs 10 ETH\n",
    "    usdc_lp_amount = (usdc_pool_0 / eth_pool_0) * eth_lp_amount\n",
    "    logger.info(f'User 1 LPs {eth_lp_amount} ETH. User 2 LPs {usdc_lp_amount} USDC.')\n",
    "    user_pool_share = eth_lp_amount / (eth_pool_0 + eth_lp_amount)\n",
    "    logger.info(f'Users together own {user_pool_share * 100}% of the total pool')\n",
    "\n",
    "     # 11% is a conservative APR estimate\n",
    "     # 4.5% net APR is a high estimate\n",
    "\n",
    "    # There are multiple ways of generating a random final state. This is a simple one.\n",
    "    # Allow the final USDC (and ETH) pool amount to be up to 50% different from the current one\n",
    "    # We sample uniformly from this distribution to deliberately oversample unlikely big changes\n",
    "    # (since those are the potentially problematic edge cases)\n",
    "    n = 1000 # Number of samples\n",
    "    diff_threshold = 0.4\n",
    "\n",
    "    usdc_pool_f_arr = np.rint(usdc_pool_0 * (2 * np.random.random(n) * diff_threshold - diff_threshold + 1))\n",
    "    eth_pool_f_arr = np.rint(eth_pool_0 * (2 * np.random.random(n) * diff_threshold - diff_threshold + 1))\n",
    "\n",
    "    _len = 4\n",
    "    logger.info('First few simulated end results:')\n",
    "    logger.info(f'USDC pool amounts = {usdc_pool_f_arr[:_len]}')\n",
    "    logger.info(f'ETH pool amounts = {eth_pool_f_arr[:_len]}')\n",
    "    logger.info(f'ETH/USDC prices = {np.rint(usdc_pool_f_arr / eth_pool_f_arr)[:_len]}')\n",
    "    logger.info(f'Pool TVLs = ${usdc_pool_f_arr[:_len] * 2 / 1e6} million')\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Note that we can break down any change in pool composition into a single (1) add/remove liquidity + (2) swap\n",
    "    Swaps do not change a user's pool share (since fundamentally there is no change in LP). And they keep x * y = k.\n",
    "    So we pretend that the add/remove liquidity comes first (makes calculating pool share difference easy)\n",
    "    and then account for the swap that brings us to our final pool amounts.\n",
    "\n",
    "    Did some math, here is the result:\n",
    "    After the add/remove liquidity, x_mid = sqrt(price_0 * k_f) and y_mid = k_f / x_mid\n",
    "    So (x_mid - x0) liquidity is added of token1 and (y_mid - y0) of token 2 at our initial price\n",
    "    \"\"\"\n",
    "    def simulate(\n",
    "        pool_token1_initial_amount, pool_token2_initial_amount,\n",
    "        user_token1_lp_amount, user_token2_lp_amount,\n",
    "        pool_token1_final_amount, pool_token2_final_amount,\n",
    "        dex_apr, lending_token1_apr, lending_token2_apr,\n",
    "        token1_name = \"token1\", token2_name = \"token2\"\n",
    "    ):\n",
    "        initial_price = pool_token1_initial_amount / pool_token2_initial_amount\n",
    "        logger.info(f'Initial pool composition: {pool_token1_initial_amount} {token1_name}, '\n",
    "            f'{pool_token2_initial_amount} {token2_name}')\n",
    "        logger.info(f'Initial price of {token2_name}/{token1_name} = {initial_price}\\n')\n",
    "        assert(np.isclose(initial_price, user_token1_lp_amount / user_token2_lp_amount))\n",
    "        \n",
    "        pool_token1_initial_amount += user_token1_lp_amount\n",
    "        pool_token2_initial_amount += user_token2_lp_amount\n",
    "        \n",
    "        initial_user_pool_share = user_token1_lp_amount / pool_token1_initial_amount\n",
    "        assert(np.isclose(initial_user_pool_share, user_token2_lp_amount / pool_token2_initial_amount))\n",
    "        logger.info(f'1. Alice and Bob LP {user_token1_lp_amount} {token1_name} and {user_token2_lp_amount} {token2_name}')\n",
    "        logger.info(f'   Initially, they own {initial_user_pool_share * 100}% of the total pool')\n",
    "            \n",
    "        k_f = pool_token1_final_amount * pool_token2_final_amount\n",
    "        \n",
    "        # Pool composition after the add/remove liquidity and before the swap\n",
    "        pool_token1_intermediate_amount = np.sqrt(initial_price * k_f)\n",
    "        pool_token2_intermediate_amount = k_f / pool_token1_intermediate_amount\n",
    "        \n",
    "        token1_liquidity_delta = pool_token1_intermediate_amount - pool_token1_initial_amount\n",
    "        token2_liquidity_delta = pool_token2_intermediate_amount - pool_token2_initial_amount\n",
    "        logger.info(f'2. Other users {\"added\" if token1_liquidity_delta > 0 else \"removed\"} liquidity: '\n",
    "            f'{np.round(np.abs(token1_liquidity_delta), 2)} {token1_name}, '\n",
    "            f'{np.round(np.abs(token2_liquidity_delta), 2)} {token2_name}')\n",
    "        \n",
    "        token1_swap_delta = pool_token1_final_amount - pool_token1_intermediate_amount\n",
    "        token2_swap_delta = pool_token2_final_amount - pool_token2_intermediate_amount\n",
    "        final_price = pool_token1_final_amount / pool_token2_final_amount\n",
    "        logger.info(f'3. Other users {\"sold\" if token1_swap_delta > 0 else \"bought\"} '\n",
    "            f'{np.round(np.abs(token1_swap_delta), 2)} {token1_name} for '\n",
    "            f'{np.round(np.abs(token2_swap_delta))} {token2_name}\\n')\n",
    "        logger.info(f'Final pool composition: {pool_token1_final_amount} {token1_name}, '\n",
    "        f'{pool_token2_final_amount} {token2_name}')\n",
    "        logger.info(f'Final price of {token2_name}/{token1_name} = {np.round(final_price, 2)}')\n",
    "        final_user_pool_share = user_token1_lp_amount / pool_token1_intermediate_amount\n",
    "        assert(np.isclose(final_user_pool_share, user_token2_lp_amount / pool_token2_intermediate_amount))\n",
    "        logger.info(f'Ultimately, the specified LPs own {final_user_pool_share * 100}% of the total pool\\n')\n",
    "\n",
    "        # Specific profit analysis below\n",
    "        lending_token1_out = user_token1_lp_amount * (1 + lending_token1_apr)\n",
    "        lending_token2_out = user_token2_lp_amount * (1 + lending_token2_apr)\n",
    "        logger.info(f'Benchmark = ReactorFusion pools: Assume {lending_token1_apr * 100}% APR on'\n",
    "            f' {token1_name} and {lending_token2_apr * 100}% APR on {token2_name}')\n",
    "        logger.info(f'Simply lending in ReactorFusion would yield '\n",
    "            f'{np.round(lending_token1_out, 2)} {token1_name} and '\n",
    "            f'{np.round(lending_token2_out, 2)} {token2_name}\\n')\n",
    "        \n",
    "        logger.info(f'Assume a conservative {dex_apr * 100}% APR on the SyncSwap {token1_name}/{token2_name} pool')\n",
    "        withdraw_token1_amount = final_user_pool_share * pool_token1_final_amount * (1 + syncswap_apr)\n",
    "        withdraw_token2_amount = final_user_pool_share * pool_token2_final_amount * (1 + syncswap_apr)\n",
    "        unbalanced_token1_amount = withdraw_token1_amount\n",
    "        unbalanced_token2_amount = withdraw_token2_amount\n",
    "        logger.info(f'Without a rebalancing mechanism, Alice and Bob withdraw '\n",
    "            f'{np.round(withdraw_token1_amount, 2)} {token1_name}'\n",
    "            f' and {np.round(withdraw_token2_amount, 2)} {token2_name} from SyncSwap\\n')\n",
    "        \n",
    "        # Now we will swap token1 for token2 or vice versa so that both sides are the same percentage better\n",
    "        # than our benchmark. For example, if the benchmark yield is 1 ETH and 2000 USDC, we will swap so that\n",
    "        # we are at 1.03 ETH and 2060 USDC (approximately)\n",
    "        # https://www.wolframalpha.com/input?i2d=true&i=solve+for+d+in+%5C%2840%29s+-+d%5C%2841%29b++%3D+%5C%2840%29t+%2B++Divide%5B%5C%2840%291+-+0.003%5C%2841%29+*+d+*+y%2Cx%5D+%5C%2841%29a\n",
    "        will_swap_token1_for_token2 = withdraw_token1_amount > lending_token1_out and \\\n",
    "            withdraw_token1_amount / lending_token1_out > withdraw_token2_amount / lending_token2_out\n",
    "        will_swap_token2_for_token1 = withdraw_token2_amount > lending_token2_out and \\\n",
    "            withdraw_token2_amount / lending_token2_out > withdraw_token1_amount / lending_token1_out\n",
    "        if (will_swap_token1_for_token2):\n",
    "            # This equation is a good approximation (just doesn't account for price impact,\n",
    "            # so we very reasonably assume the swap is small compared to the size of the pool),\n",
    "            # but we should sanity check it before doing the swap\n",
    "            numer = 1000 * pool_token1_final_amount * \\\n",
    "                (lending_token2_out * withdraw_token1_amount - lending_token1_out * withdraw_token2_amount)\n",
    "            denom = (997 * lending_token1_out * pool_token2_final_amount +\n",
    "                    1000 * lending_token2_out * pool_token1_final_amount)\n",
    "            swap_token1_amount_in = numer / denom\n",
    "            # We will query pool balances in the smart contract call before doing the swap,\n",
    "            # so there must be zero slippage\n",
    "            estimated_token2_amount_out = 0.997 * swap_token1_amount_in * pool_token2_final_amount \\\n",
    "                / (pool_token1_final_amount + swap_token1_amount_in)\n",
    "            withdraw_token1_amount -= swap_token1_amount_in\n",
    "            withdraw_token2_amount += estimated_token2_amount_out\n",
    "            logger.info(f'Rebalancing mechanism: We swap {np.round(swap_token1_amount_in, 2)} {token1_name} '\n",
    "                f'for {np.round(estimated_token2_amount_out, 2)} {token2_name}')\n",
    "        elif will_swap_token2_for_token1:\n",
    "            numer = 1000 * pool_token2_final_amount * \\\n",
    "                (lending_token1_out * withdraw_token2_amount - lending_token2_out * withdraw_token1_amount)\n",
    "            denom = (997 * lending_token2_out * pool_token1_final_amount +\n",
    "                    1000 * lending_token1_out * pool_token2_final_amount)\n",
    "            swap_token2_amount_in = numer / denom\n",
    "            # We will query pool balances in the smart contract call before doing the swap,\n",
    "            # so there must be zero slippage\n",
    "            estimated_token1_amount_out = 0.997 * swap_token2_amount_in * pool_token1_final_amount \\\n",
    "                / (pool_token2_final_amount + swap_token2_amount_in)\n",
    "            withdraw_token1_amount += estimated_token1_amount_out\n",
    "            withdraw_token2_amount -= swap_token2_amount_in\n",
    "            logger.info(f'Rebalancing mechanism: We swap {np.round(swap_token2_amount_in, 2)} {token2_name} '\n",
    "                f'for {np.round(estimated_token1_amount_out, 2)} {token1_name}')\n",
    "        else:\n",
    "            logger.info('Unexpected, the benchmark performed better on both tokens. '\n",
    "                'Is the DEX APY higher than the lending APY?')\n",
    "            assert(false)\n",
    "        logger.info(f'After the rebalancing mechanism, Alice withdraws '\n",
    "        f'{np.round(withdraw_token1_amount, 2)} {token1_name}'\n",
    "        f' and Bob withdraws {np.round(withdraw_token2_amount, 2)} {token2_name} from SyncSwap\\n')\n",
    "        logger.info('---------------')\n",
    "        logger.info(f'Alice received {(withdraw_token1_amount / lending_token1_out - 1) * 100}% '\n",
    "            f'more {token1_name} via single-sided SyncSwap LP than with ReactorFusion lending {token1_name} alone.')\n",
    "        logger.info(f'Bob received {(withdraw_token2_amount / lending_token2_out - 1) * 100}% '\n",
    "        f'more {token2_name} via single-sided SyncSwap LP than with ReactorFusion lending {token2_name} alone.')\n",
    "        logger.info('---------------\\n')\n",
    "        misc_data = {\n",
    "            \"withdraw_token1_amount\": withdraw_token1_amount,\n",
    "            \"unbalanced_withdraw_token1_amount\": unbalanced_token1_amount,\n",
    "            \"withdraw_token2_amount\": withdraw_token2_amount,\n",
    "            \"unbalanced_withdraw_token2_amount\": unbalanced_token2_amount,\n",
    "            \"final_price\": final_price\n",
    "        }\n",
    "        token1_dollar_value_over_dual_lp = (withdraw_token1_amount - unbalanced_token1_amount)\n",
    "        token2_dollar_value_over_dual_lp = withdraw_token2_amount / unbalanced_token2_amount * final_price\n",
    "        return withdraw_token1_amount / lending_token1_out - 1, withdraw_token2_amount / lending_token2_out - 1, \\\n",
    "            withdraw_token1_amount / user_token1_lp_amount - 1, withdraw_token2_amount / user_token2_lp_amount - 1, \\\n",
    "            misc_data\n",
    "    \n",
    "    simulate(usdc_pool_0, eth_pool_0, usdc_lp_amount, eth_lp_amount,\n",
    "          usdc_pool_f_arr[0], eth_pool_f_arr[0],\n",
    "          syncswap_apr, reactorfusion_usdc_apr, reactorfusion_eth_apr,\n",
    "          \"USDC\", \"ETH\")\n",
    "    \n",
    "\n",
    "lping_vs_lending_analysis()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def price_impact_analysis(\n",
    "    weeks=52,\n",
    "    initial_weekly_transactions=1000,\n",
    "    initial_circulating_supply=1000000,\n",
    "    initial_price=1.0,\n",
    "    growth_rate=0.05,\n",
    "    growth_rate_decay=0.01,\n",
    "    buy_transactions=0.1,\n",
    "    sell_transactions=0.05,\n",
    "    D0=10000,\n",
    "    kc=0.1,\n",
    "    kl=0.05,\n",
    "    Delta_Dc=500,\n",
    "    Delta_Dl=300,\n",
    "    epsilon_d=0.8,\n",
    "    epsilon_s=0.5\n",
    "):\n",
    "    # Initialize data arrays\n",
    "    weekly_transactions = np.zeros(weeks)\n",
    "    tokens_locked = np.zeros(weeks)\n",
    "    tokens_released = np.zeros(weeks)\n",
    "    net_supply_change = np.zeros(weeks)\n",
    "    circulating_supply = np.zeros(weeks)\n",
    "    token_price = np.zeros(weeks)\n",
    "    net_demand_change = np.zeros(weeks)\n",
    "    price_change = np.zeros(weeks)\n",
    "        \n",
    "    # Initial conditions\n",
    "    weekly_transactions[0] = initial_weekly_transactions\n",
    "    circulating_supply[0] = initial_circulating_supply\n",
    "    token_price[0] = initial_price\n",
    "    net_demand_change[0] = D0 + kc * Delta_Dc + kl * Delta_Dl\n",
    "    price_change[0]=0\n",
    "\n",
    "    # Model each week\n",
    "    for week in range(1, weeks):\n",
    "\n",
    "        weekly_transactions[week] = weekly_transactions[week - 1] * (1 + growth_rate)\n",
    "        tokens_locked[week] = weekly_transactions[week] * buy_transactions\n",
    "        tokens_released[week] = weekly_transactions[week] * sell_transactions\n",
    "        net_supply_change[week] = tokens_released[week] - tokens_locked[week] \n",
    "        circulating_supply[week] = circulating_supply[week - 1] + net_supply_change[week]\n",
    "\n",
    "        # Recalculate demand increase for the week\n",
    "        Delta_Dc *= (1 + growth_rate)  \n",
    "        Delta_Dl *= (1 + growth_rate)  \n",
    "        net_demand_change[week] = D0 + kc * Delta_Dc + kl * Delta_Dl\n",
    "\n",
    "        # Price adjustment based on the change in demand and circulating supply\n",
    "        \n",
    "        delta_D = net_demand_change[week] - net_demand_change[week - 1]\n",
    "        delta_S = net_supply_change[week]  \n",
    "        \n",
    "        # Price adjustment using price elasticities\n",
    "        price_change_due_to_demand = (delta_D / (net_demand_change[week - 1] * epsilon_d))\n",
    "        price_change_due_to_supply = (delta_S / ((circulating_supply[week - 1] + delta_S) * epsilon_s))\n",
    "\n",
    "        # Net price change and new price calculation\n",
    "        price_change[week] = price_change_due_to_demand + price_change_due_to_supply\n",
    "        token_price[week] = token_price[week - 1] * (1 + price_change[week])\n",
    "\n",
    "        # Growth rate decay on weekly basis\n",
    "        growth_rate = growth_rate * (1 - week * growth_rate_decay)\n",
    "\n",
    "    # Update results DataFrame with new calculations\n",
    "    updated_results = pd.DataFrame({\n",
    "        \"Week\": np.arange(weeks),\n",
    "        \"Transactions\": weekly_transactions,\n",
    "        \"Tokens Locked\": tokens_locked,\n",
    "        \"Tokens Released\": tokens_released,\n",
    "        \"Weekly Supply Change\": net_supply_change,\n",
    "        \"Weekly Demand Change\": net_demand_change,\n",
    "        \"Circulating Supply\": circulating_supply,\n",
    "        \"Percentage change Price\": price_change,\n",
    "        \"Token Price\": token_price,\n",
    "    })\n",
    "\n",
    "    # Plotting the results with the revised price adjustment\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(10, 15))\n",
    "\n",
    "    # Plot for Circulating Supply\n",
    "    axs[0].plot(updated_results['Week'], updated_results['Circulating Supply'], label='Circulating Supply', color='blue')\n",
    "    axs[0].set_title('Effective Circulating Supply Over Time')\n",
    "    axs[0].set_xlabel('Week')\n",
    "    axs[0].set_ylabel('Effective Circulating Supply')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot for Token Price\n",
    "    axs[1].plot(updated_results['Week'], updated_results['Token Price'], label='Token Price', color='red')\n",
    "    axs[1].set_title('Token Price Over Time')\n",
    "    axs[1].set_xlabel('Week')\n",
    "    axs[1].set_ylabel('Token Price (USD)')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Plot for Demand Increase\n",
    "    axs[2].plot(updated_results['Week'], updated_results['Weekly Demand Change'], label='Weekly Demand Change', color='green')\n",
    "    axs[2].set_title('Weekly Demand Change Over Time')\n",
    "    axs[2].set_xlabel('Week')\n",
    "    axs[2].set_ylabel('Demand Change')\n",
    "    axs[2].grid(True)\n",
    "\n",
    "    # Plot for Tokens Locked\n",
    "    axs[3].plot(updated_results['Week'], updated_results['Tokens Locked'], label='Tokens Locked', color='green')\n",
    "    axs[3].set_title('Token Locked in Sinks Over Time')\n",
    "    axs[3].set_xlabel('Week')\n",
    "    axs[3].set_ylabel('Tokens Locked')\n",
    "    axs[3].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return updated_results\n",
    "\n",
    "# Example usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = price_impact_analysis()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ILP Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': \"[Errno 2] No such file or directory: '/mnt/c/Users/hijaz tr/Desktop/cadCADProject1/Intelligent-Liquidity-Provisioning-Framework-V1'\"}\n",
      "{'error': \"'list' object has no attribute 'tolist'\"}\n",
      "{'error': \"'list' object has no attribute 'tolist'\"}\n",
      "{'error': \"'list' object has no attribute 'tolist'\"}\n",
      "{'error': \"'list' object has no attribute 'tolist'\"}\n",
      "{'strategy_action': 'Add new liquidity position', 'ddpg_action': {'price_lower': 2936.108644564212, 'price_upper': 4536.0468162693805}, 'ppo_action': {'price_lower': 3044.989546812123, 'price_upper': 3317.2662559559903}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def initialize_script(base_path, reset_env_var=True):\n",
    "    url = 'http://127.0.0.1:8000/initialize_script/'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"base_path\": base_path,\n",
    "        \"reset_env_var\": reset_env_var\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()\n",
    "\n",
    "def train_ddpg(max_steps=2, n_episodes=2, model_name=\"model_storage/ddpg/ddpg_fazool\",\n",
    "               alpha=0.001, beta=0.001, tau=0.8, batch_size=50, training=True,\n",
    "               agent_budget_usd=10000, use_running_statistics=False):\n",
    "    url = 'http://127.0.0.1:8000/train_ddpg/'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"max_steps\": max_steps,\n",
    "        \"n_episodes\": n_episodes,\n",
    "        \"model_name\": model_name,\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\": beta,\n",
    "        \"tau\": tau,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"training\": training,\n",
    "        \"agent_budget_usd\": agent_budget_usd,\n",
    "        \"use_running_statistics\": use_running_statistics\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()\n",
    "\n",
    "def evaluate_ddpg(eval_steps=2, eval_episodes=2, model_name=\"model_storage/ddpg/ddpg_fazool\",\n",
    "                  percentage_range=0.6, agent_budget_usd=10000, use_running_statistics=False):\n",
    "    url = 'http://127.0.0.1:8000/evaluate_ddpg/'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"eval_steps\": eval_steps,\n",
    "        \"eval_episodes\": eval_episodes,\n",
    "        \"model_name\": model_name,\n",
    "        \"percentage_range\": percentage_range,\n",
    "        \"agent_budget_usd\": agent_budget_usd,\n",
    "        \"use_running_statistics\": use_running_statistics\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()\n",
    "\n",
    "def train_ppo(max_steps=2, n_episodes=2, model_name=\"model_storage/ppo/ppo2_fazool22\",\n",
    "              buffer_size=5, n_epochs=20, gamma=0.5, alpha=0.001, gae_lambda=0.75,\n",
    "              policy_clip=0.6, max_grad_norm=0.6, agent_budget_usd=10000, use_running_statistics=False,\n",
    "              action_transform=\"linear\"):\n",
    "    url = 'http://127.0.0.1:8000/train_ppo/'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"max_steps\": max_steps,\n",
    "        \"n_episodes\": n_episodes,\n",
    "        \"model_name\": model_name,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"gamma\": gamma,\n",
    "        \"alpha\": alpha,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"policy_clip\": policy_clip,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"agent_budget_usd\": agent_budget_usd,\n",
    "        \"use_running_statistics\": use_running_statistics,\n",
    "        \"action_transform\": action_transform\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()\n",
    "\n",
    "def evaluate_ppo(eval_steps=2, eval_episodes=2, model_name=\"model_storage/ppo/ppo2_fazool\",\n",
    "                 percentage_range=0.5, agent_budget_usd=10000, use_running_statistics=False,\n",
    "                 action_transform=\"linear\"):\n",
    "    url = 'http://127.0.0.1:8000/evaluate_ppo/'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"eval_steps\": eval_steps,\n",
    "        \"eval_episodes\": eval_episodes,\n",
    "        \"model_name\": model_name,\n",
    "        \"percentage_range\": percentage_range,\n",
    "        \"agent_budget_usd\": agent_budget_usd,\n",
    "        \"use_running_statistics\": use_running_statistics,\n",
    "        \"action_transform\": action_transform\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()\n",
    "\n",
    "def inference(pool_state, user_preferences, pool_id=\"0x4e68ccd3e89f51c3074ca5072bbac773960dfa36\",\n",
    "              ddpg_agent_path=\"model_storage/ddpg/ddpg_1\",\n",
    "              ppo_agent_path=\"model_storage/ppo/lstm_actor_critic_batch_norm\"):\n",
    "    url = 'http://127.0.0.1:8000/inference/'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"pool_state\": pool_state,\n",
    "        \"user_preferences\": user_preferences,\n",
    "        \"pool_id\": pool_id,\n",
    "        \"ddpg_agent_path\": ddpg_agent_path,\n",
    "        \"ppo_agent_path\": ppo_agent_path\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "base_path = \"/mnt/d/Code/tempest/Intelligent-Liquidity-Provisioning-Framework-V2\"\n",
    "initialize_response = initialize_script(base_path)\n",
    "print(initialize_response)\n",
    "\n",
    "train_ddpg_response = train_ddpg()\n",
    "print(train_ddpg_response)\n",
    "\n",
    "evaluate_ddpg_response = evaluate_ddpg()\n",
    "print(evaluate_ddpg_response)\n",
    "\n",
    "train_ppo_response = train_ppo()\n",
    "print(train_ppo_response)\n",
    "\n",
    "evaluate_ppo_response = evaluate_ppo()\n",
    "print(evaluate_ppo_response)\n",
    "\n",
    "pool_state = {\n",
    "    \"current_profit\": 500,\n",
    "    \"price_out_of_range\": False,\n",
    "    \"time_since_last_adjustment\": 40000,\n",
    "    \"pool_volatility\": 0.2\n",
    "}\n",
    "user_preferences = {\n",
    "    \"risk_tolerance\": {\"profit_taking\": 50, \"stop_loss\": -500},\n",
    "    \"investment_horizon\": 7,\n",
    "    \"liquidity_preference\": {\"adjust_on_price_out_of_range\": True},\n",
    "    \"risk_aversion_threshold\": 0.1,\n",
    "    \"user_status\": \"new_user\"\n",
    "}\n",
    "inference_response = inference(pool_state, user_preferences)\n",
    "print(inference_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniswap V3 simulator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokengpt (venv)",
   "language": "python",
   "name": "tgpt_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
