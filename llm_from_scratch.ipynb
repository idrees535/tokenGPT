{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Neural network modules and functions from PyTorch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for plotting Loss etc.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Time module for tracking execution time\n",
    "import time\n",
    "\n",
    "# Pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# urllib for handling URL requests (Downloading Dataset)\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urllib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtinyshakespeare.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Execute the download\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43murllib\u001b[49m\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlretrieve(url, file_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'urllib' is not defined"
     ]
    }
   ],
   "source": [
    "# The URL of the raw text file on GitHub\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "# The file name for local storage\n",
    "file_name = \"tinyshakespeare.txt\"\n",
    "\n",
    "# Execute the download\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the dataset\n",
    "lines = open(\"tinyshakespeare.txt\", 'r').read()\n",
    "\n",
    "# Create a sorted list of unique characters in the dataset\n",
    "vocab = sorted(list(set(lines)))\n",
    "\n",
    "# Display the first 10 characters in the vocabulary list\n",
    "print('Printing the first 10 characters of the vocab list:', vocab[:10])\n",
    "\n",
    "# Output the total number of characters in our dataset (Vocabulary Size)\n",
    "print('Total number of characters in our dataset (Vocabulary Size):', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping integers to characters (itos)\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "# Mapping characters to integers (stoi)\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode function: Converts a string to a list of integers using the mapping stoi\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "# Decode function: Converts a list of integers back to a string using the mapping itos\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Example: Encode the string \"hello\" and then decode the result\n",
    "decode(encode(\"morning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into a torch tensor with specified data type (dtype)\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "\n",
    "# Display the shape of the resulting tensor\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get batches for training, validation, or testing\n",
    "def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):\n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    # Determine which split to use\n",
    "    batch_data = train\n",
    "    if split == 'val':\n",
    "        batch_data = val\n",
    "    if split == 'test':\n",
    "        batch_data = test\n",
    "\n",
    "    # Pick random starting points within the data\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "\n",
    "    # Create input sequences (x) and corresponding target sequences (y)\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration object for model parameters\n",
    "MASTER_CONFIG = {\n",
    "    # Adding parameters later\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the MASTER_CONFIG with batch_size and context_window parameters\n",
    "MASTER_CONFIG.update({\n",
    "    'batch_size': 8,          # Number of batches to be processed at each random split\n",
    "    'context_window': 16      # Number of characters in each input (x) and target (y) sequence of each batch\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain batches for training using the specified batch size and context window\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Decode the sequences to obtain the corresponding text representations\n",
    "decoded_samples = [(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]\n",
    "\n",
    "# Print the random sample\n",
    "print(decoded_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Don't compute gradients for this function\n",
    "def evaluate_loss(model, config=MASTER_CONFIG):\n",
    "    # Placeholder for the evaluation results\n",
    "    out = {}\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate through training and validation splits\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        # Placeholder for individual losses\n",
    "        losses = []\n",
    "\n",
    "        # Generate 10 batches for evaluation\n",
    "        for _ in range(10):\n",
    "            # Get input sequences (xb) and target sequences (yb)\n",
    "            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
    "            \n",
    "            # Perform model inference and calculate the loss\n",
    "            _, loss = model(xb, yb)\n",
    "            \n",
    "            # Append the loss to the list\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Calculate the mean loss for the split and store it in the output dictionary\n",
    "        out[split] = np.mean(losses)\n",
    "    \n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a basic neural network class\n",
    "class SimpleBrokenModel(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding layer to convert character indices to vectors (vocab size: 65)\n",
    "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "\n",
    "        # Linear layers for modeling relationships between features\n",
    "        # (to be updated with SwiGLU activation function as in LLaMA)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            nn.ReLU(),  # Currently using ReLU, will be replaced with SwiGLU as in LLaMA\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        # Print the total number of model parameters\n",
    "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "            # Embedding layer converts character indices to vectors\n",
    "            x = self.embedding(idx)\n",
    "            \n",
    "            # Linear layers for modeling relationships between features\n",
    "            a = self.linear(x)\n",
    "            \n",
    "            # Apply softmax activation to obtain probability distribution\n",
    "            logits = F.softmax(a, dim=-1)\n",
    "\n",
    "            # If targets are provided, calculate and return the cross-entropy loss\n",
    "            if targets is not None:\n",
    "                # Reshape logits and targets for cross-entropy calculation\n",
    "                loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "                return logits, loss\n",
    "\n",
    "            # If targets are not provided, return the logits\n",
    "            else:\n",
    "                return logits\n",
    "\n",
    "        # Print the total number of model parameters\n",
    "        print(\"Model parameters:\", sum([m.numel() for m in self.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MASTER_CONFIG with the dimension of linear layers (128)\n",
    "MASTER_CONFIG.update({\n",
    "    'd_model': 128,\n",
    "})\n",
    "\n",
    "# Instantiate the SimpleBrokenModel using the updated MASTER_CONFIG\n",
    "model = SimpleBrokenModel(MASTER_CONFIG)\n",
    "\n",
    "# Print the total number of parameters in the model\n",
    "print(\"Total number of parameters in the Simple Neural Network Model:\", sum([m.numel() for m in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain batches for training using the specified batch size and context window\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = model(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MASTER_CONFIG with training parameters\n",
    "MASTER_CONFIG.update({\n",
    "    'epochs': 1000,          # Number of training epochs\n",
    "    'log_interval': 10,      # Log information every 10 batches during training\n",
    "    'batch_size': 32,        # Increase batch size to 32\n",
    "})\n",
    "\n",
    "# Instantiate the SimpleBrokenModel with updated configuration\n",
    "model = SimpleBrokenModel(MASTER_CONFIG)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),      # Pass the model parameters to the optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training\n",
    "def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):\n",
    "    # Placeholder for storing losses\n",
    "    losses = []\n",
    "    \n",
    "    # Start tracking time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through epochs\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Obtain batches for training\n",
    "        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
    "\n",
    "        # Forward pass through the model to calculate logits and loss\n",
    "        logits, loss = model(xs, targets=ys)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # If a learning rate scheduler is provided, adjust the learning rate\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Log progress every specified interval\n",
    "        if epoch % config['log_interval'] == 0:\n",
    "            # Calculate batch time\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate loss on validation set\n",
    "            x = evaluate_loss(model)\n",
    "            \n",
    "            # Store the validation loss\n",
    "            losses += [x]\n",
    "            \n",
    "            # Print progress logs if specified\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
    "                \n",
    "            # Reset the timer\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Print learning rate if a scheduler is provided\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())\n",
    "\n",
    "    # Print the final validation loss\n",
    "    print(\"Validation loss: \", losses[-1]['val'])\n",
    "    \n",
    "    # Plot the training and validation loss curves\n",
    "    return pd.DataFrame(losses).plot()\n",
    "\n",
    "# Execute the training process\n",
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate function for text generation using the trained model\n",
    "def generate(model, config=MASTER_CONFIG, max_new_tokens=30):\n",
    "    idx = torch.zeros(5, 1).long()\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Call the model\n",
    "        logits = model(idx[:, -config['context_window']:])\n",
    "        last_time_step_logits = logits[\n",
    "            :, -1, :\n",
    "        ]  # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )  # sample from the distribution to get the next token\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence\n",
    "    return [decode(x) for x in idx.tolist()]\n",
    "\n",
    "# Generate text using the trained model\n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Embedding layer for token representations\n",
    "        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
    "        # Sequential block of LlamaBlocks based on the specified number of layers\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config['n_layers'])])\n",
    "        )\n",
    "        # Feedforward network (FFN) for final output\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config['d_model'], config['d_model']),\n",
    "            SwiGLU(config['d_model']),\n",
    "            nn.Linear(config['d_model'], config['vocab_size']),\n",
    "        )\n",
    "\n",
    "        # Print total number of parameters in the model\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # Input token indices are passed through the embedding layer\n",
    "        x = self.embeddings(idx)\n",
    "        # Process the input through the LlamaBlocks\n",
    "        x = self.llama_blocks(x)\n",
    "        # Pass the processed input through the final FFN for output logits\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        # If targets are not provided, return only the logits\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        # If targets are provided, compute and return the cross-entropy loss\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head, SwiGLU, N_layers)\n",
    "llama = Llama(MASTER_CONFIG)\n",
    "\n",
    "# Obtain batches for training\n",
    "xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])\n",
    "\n",
    "# Calculate logits and loss using the model\n",
    "logits, loss = llama(xs, ys)\n",
    "\n",
    "# Define the Adam optimizer for model parameters\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "\n",
    "# Train the model\n",
    "train(llama, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the number of epochs in the configuration\n",
    "MASTER_CONFIG.update({\n",
    "    'epochs': 10000,\n",
    "})\n",
    "# Train the LLaMA model for the specified number of epochs\n",
    "train(llama, optimizer, scheduler=None, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(llama, 'llama_model.pth')\n",
    "\n",
    "# If you want to save only the model parameters\n",
    "torch.save(llama.state_dict(), 'llama_model_params.pth')\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Assuming Llama is your PyTorch model\n",
    "llama_config = GPT2Config.from_dict(MASTER_CONFIG)\n",
    "llama_transformers = GPT2LMHeadModel(config=llama_config)\n",
    "llama_transformers.load_state_dict(llama.state_dict())\n",
    "\n",
    "# Specify the directory where you want to save the model\n",
    "output_dir = \"llama_model_transformers\"\n",
    "\n",
    "# Save the model and configuration\n",
    "llama_transformers.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Specify the directory where the model was saved\n",
    "output_dir = \"llama_model_transformers\"\n",
    "\n",
    "# Load the model and configuration\n",
    "llama_transformers = GPT2LMHeadModel.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgpt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
