{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TokenGPT Features\n",
    "LLM fine tuned on token data (Reaserch papers, whitepapers, articles,tools etc)\n",
    "Implement RAG for more factual repsonses backed by data (whiteapers, news etc)\n",
    "Implement Tools: QTM, Uniswap Subgraph Quesreis, Toeknspice, cadCAD, Dune etc\n",
    "Provide out of the box customized chatbots for dApps i.e UniswapV3 chatbot which has the context of uniswap live data form subgraph, whitepapers etc and provides responses to User queries i.e How to deploy a new pool, what are differnt fee tiers etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def generate_text(prompt, model_name='gpt2', max_length=100):\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Encode the input text to tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Load pre-trained model\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Generate text until the output length (which includes the context length) reaches 100\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "    # Decode the output tensor to a string\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHo was last president of usa- tion. He was a man of great integrity and integrity. He was a man of great integrity and integrity. He was a man of great integrity and integrity. He was a man of great integrity and integrity. He was a man of great integrity and integrity. He was a man of great integrity and integrity. He was a man of great integrity and integrity. He was a man of great integrity and integrity. He was a man of great integrity and integrity\n"
     ]
    }
   ],
   "source": [
    "prompt = \"WHo was last president of usa\"\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Query: The user enters a query or a request.\n",
    "\n",
    "Planner Agentâ€™s Prompt Engineering: Based on the user query, the Planner Agent crafts a specialized prompt. This process involves:\n",
    "\n",
    "Incorporating relevant domain-specific information from memory.\n",
    "Detailing the tools available in the toolkit, including their names and capabilities.\n",
    "Formulating a request to the LLM (in this case, GPT-3.5) to generate a step-by-step plan. This request will include instructions on selecting the appropriate tools from the toolkit and determining their parameters based on the user's query.\n",
    "Interaction with GPT-3.5: The engineered prompt is then passed to GPT-3.5. The LLM uses the information within the prompt to understand the context and the available tools, generating a detailed task list accordingly.\n",
    "\n",
    "Fine-Tuning for Efficiency: To optimize this process, the model can be fine-tuned with examples where the engineered prompt is given and the desired structured response (task list) is provided. This fine-tuning helps the model better understand the types of responses that are useful in the context of token system simulations.\n",
    "\n",
    "Output Generation: GPT-3.5, after processing the prompt and utilizing the information from the toolkit and memory, outputs a sequence of tasks. These tasks indicate which tools to use and with what parameters, tailored to the specific user query.\n",
    "\n",
    "Executor Agent's Role: This task list is then taken up by the Executor Agent, which executes each task, using the specified tools and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import openai\n",
    "from chatbot import CadCAD_GPT_Chatbot\n",
    "from planner_agent import PlannerAgent\n",
    "from executor_agent import ExecutorAgent\n",
    "from toolkit import Toolkit\n",
    "from memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CadCAD_GPT_Chatbot:\n",
    "    def __init__(self, openai_key, model, simulation, experiment, docs):\n",
    "        self.openai_key = openai_key\n",
    "        self.model = model\n",
    "        self.simulation = simulation\n",
    "        self.experiment = experiment\n",
    "        self.docs = docs\n",
    "        self.planner_agent = PlannerAgent(openai_key)\n",
    "        self.executor_agent = ExecutorAgent(openai_key)\n",
    "\n",
    "    def process_user_query(self, user_query):\n",
    "        task_list = self.planner_agent.create_task_list(user_query)\n",
    "        return self.executor_agent.execute_tasks(task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toolkit:\n",
    "    # Define various tools for simulations, analysis, and data manipulation\n",
    "    pass\n",
    "class Memory:\n",
    "    # Manage long-term and short-term memory for the system\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "class PlannerAgent:\n",
    "    def __init__(self, openai_key, toolkit, memory):\n",
    "        self.openai_key = openai_key\n",
    "        self.toolkit = toolkit\n",
    "        self.memory = memory\n",
    "        openai.api_key = self.openai_key\n",
    "\n",
    "    def generate_task_list(self, user_query):\n",
    "        context = self._prepare_context(user_query)\n",
    "        response = openai.Completion.create(prompt=context, model=\"gpt-3.5-turbo\", max_tokens=150)\n",
    "        task_list = self._parse_response_into_tasks(response['choices'][0]['text'])\n",
    "        return task_list\n",
    "\n",
    "    def _prepare_context(self, user_query):\n",
    "        # Prepare the prompt by integrating user query, toolkit, and memory\n",
    "        context = f\"User query: {user_query}\\n\\n\"\n",
    "        context += \"Available tools: \" + \", \".join(self.toolkit.keys()) + \"\\n\"\n",
    "        context += \"Memory data: \" + str(self.memory) + \"\\n\\n\"\n",
    "        context += \"Generate a sequence of tasks to answer the query:\"\n",
    "        return context\n",
    "\n",
    "    def _parse_response_into_tasks(self, response_text):\n",
    "        # Extract the sequence of tasks from the LLM's response\n",
    "        # This parsing depends on how the LLM structures its response\n",
    "        return task_list\n",
    "\n",
    "# Example usage\n",
    "toolkit = {\n",
    "    'change_param': change_param_function,\n",
    "    'plotter': plotter_function,\n",
    "    # More tools...\n",
    "}\n",
    "\n",
    "memory = {...}  # Initialize memory\n",
    "\n",
    "planner_agent = PlannerAgent(openai_key=\"your-api-key\", toolkit=toolkit, memory=memory)\n",
    "user_query = \"How can I optimize the liquidity provision in a DeFi protocol?\"\n",
    "task_list = planner_agent.generate_task_list(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "class ExecutorAgent:\n",
    "    def __init__(self, openai_key, toolkit, memory):\n",
    "        self.openai_key = openai_key\n",
    "        self.toolkit = toolkit\n",
    "        self.memory = memory\n",
    "        openai.api_key = self.openai_key\n",
    "\n",
    "    def execute_tasks(self, task_list):\n",
    "        results = []\n",
    "        for task in task_list:\n",
    "            tool_name, args = self._interpret_task(task)\n",
    "            if tool_name in self.toolkit:\n",
    "                tool_function = self.toolkit[tool_name]\n",
    "                result = tool_function(*args)\n",
    "                results.append(result)\n",
    "                self._update_memory(result)\n",
    "            else:\n",
    "                raise ValueError(f\"Tool {tool_name} not found in toolkit.\")\n",
    "        return results\n",
    "\n",
    "    def _interpret_task(self, task):\n",
    "        # Interpret the task to figure out the tool and arguments\n",
    "        response = openai.Completion.create(prompt=task, model=\"gpt-3.5-turbo\", max_tokens=50)\n",
    "        response_data = response['choices'][0]['text']\n",
    "        tool_name, args = self._parse_response_data(response_data)\n",
    "        return tool_name, args\n",
    "\n",
    "    def _parse_response_data(self, response_data):\n",
    "        # Logic to extract tool name and arguments from LLM response\n",
    "        # This part should be customized based on how the data is formatted in the response\n",
    "        return tool_name, args\n",
    "\n",
    "    def _update_memory(self, result):\n",
    "        # Update the memory based on the result of the execution\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "toolkit = {\n",
    "    'change_param': change_param_function,\n",
    "    'plotter': plotter_function,\n",
    "    # More tools...\n",
    "}\n",
    "\n",
    "memory = {...}  # Initialize memory\n",
    "\n",
    "executor_agent = ExecutorAgent(openai_key=\"your-api-key\", toolkit=toolkit, memory=memory)\n",
    "task_list = ['Change parameter X to 0.5', 'Plot the result of the last simulation']\n",
    "results = executor_agent.execute_tasks(task_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code for Executor Agent in cadCAD GPT\n",
    "\n",
    "class ExecutorAgent:\n",
    "    def __init__(self, toolkit, memory):\n",
    "        self.toolkit = toolkit\n",
    "        self.memory = memory\n",
    "        self.short_term_memory = {}\n",
    "\n",
    "    def execute_task_list(self, task_list):\n",
    "        for task in task_list:\n",
    "            self.think(task)\n",
    "            action = self.select_action(task)\n",
    "            result = self.execute_action(action)\n",
    "            self.observe(result)\n",
    "            # Store important info in short-term memory\n",
    "            self.short_term_memory[task] = result\n",
    "\n",
    "    def think(self, task):\n",
    "        # Interpret the task and plan the execution\n",
    "        pass\n",
    "\n",
    "    def select_action(self, task):\n",
    "        # Determine which function to call and its parameters\n",
    "        function_name, args = self.parse_task_to_function_call(task)\n",
    "        return {'function': function_name, 'arguments': args}\n",
    "\n",
    "    def execute_action(self, action):\n",
    "        # Call the function from the toolkit with given arguments\n",
    "        function = self.toolkit.get(action['function'])\n",
    "        return function(*action['arguments'])\n",
    "\n",
    "    def observe(self, result):\n",
    "        # Process and understand the outcome of the action\n",
    "        pass\n",
    "\n",
    "    def parse_task_to_function_call(self, task):\n",
    "        # Translate task into a function call with parameters\n",
    "        # This would involve interpreting the toolkit and memory\n",
    "        pass\n",
    "\n",
    "# Example usage\n",
    "executor_agent = ExecutorAgent(toolkit, memory)\n",
    "executor_agent.execute_task_list(planner_agent_output_task_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    openai_key = \"YOUR_OPENAI_API_KEY\"\n",
    "    # Initialize model, simulation, experiment, and documentation\n",
    "    model = None\n",
    "    simulation = None\n",
    "    experiment = None\n",
    "    docs = None\n",
    "\n",
    "    cadcad_gpt_chatbot = CadCAD_GPT_Chatbot(openai_key, model, simulation, experiment, docs)\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"Enter your query: \")\n",
    "        response = cadcad_gpt_chatbot.process_user_query(user_query)\n",
    "        print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m new_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am having trouble connecting to Wi-Fi. Can you help?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# or whichever model you're using\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hijaz tr\\Desktop\\cadCADProject1\\abcde\\lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: The model `text-davinci-003` has been deprecated, learn more here: https://platform.openai.com/docs/deprecations"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Your API key from OpenAI\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Define a few-shot prompt with examples\n",
    "# Example 1\n",
    "example_1_question = \"My phone won't turn on. What should I do?\"\n",
    "example_1_answer = \"First, try charging your phone for at least 30 minutes. If it still won't turn on, hold down the power button for 10 seconds.\"\n",
    "\n",
    "# Example 2\n",
    "example_2_question = \"How do I reset my password?\"\n",
    "example_2_answer = \"You can reset your password by going to the settings, selecting 'Account', and then 'Reset Password'.\"\n",
    "\n",
    "# Few-shot prompt\n",
    "prompt = f\"Q: {example_1_question}\\nA: {example_1_answer}\\n\\nQ: {example_2_question}\\nA: {example_2_answer}\\n\\n\"\n",
    "\n",
    "# Add your new question here\n",
    "new_question = \"I am having trouble connecting to Wi-Fi. Can you help?\"\n",
    "prompt += f\"Q: {new_question}\\nA:\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\", # or whichever model you're using\n",
    "  prompt=prompt,\n",
    "  temperature=0.5,\n",
    "  max_tokens=100\n",
    ")\n",
    "\n",
    "print(response.choices[0].text.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abcde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
